{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusterização (Modelo Não Supervisionado K-means)\n",
    "Este notebook explora o algoritmo de aprendizado de máquina não supervisionado K-Means. <br>\n",
    "O objetivo deste método é agrupar dados em diferentes grupos (clusters), sem o uso de rótulos prévios, e buscar tendências. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Instalação de bibliotecas\n",
    "Instala bibliotecas necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas==2.1.4 numpy==1.26.4 matplotlib==3.7.5 seaborn==0.13.2 scikit-learn==1.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Inicialização de bibliotecas\n",
    "Importa as bibliotecas necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Configuração para mostrar todas as colunas no pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Configuração para exibir os gráficos diretamente no notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Importe do banco de dados via arquivo local\n",
    "Importação do DataFrame da base de dados da Unipar com a biblioteca pandas:\n",
    "\n",
    "Para rodar o notebook corretamente, será necessário importar o arquivo de banco de dados manualmente, pois ele não está incluído no repositório devido à presença de dados sensíveis.\n",
    "\n",
    "#### Passos para Importar o Banco de Dados no VS Code\n",
    "\n",
    "1. **Obtenha o Arquivo de Dados**:\n",
    "   - O arquivo `BASE DE SINISTRO UNIPAR BRADESCO.csv` deverá ser fornecido separadamente. Entre em contato com o responsável pelo projeto para receber o arquivo.\n",
    "\n",
    "2. **Posicione o Arquivo na Pasta Correta**:\n",
    "   - Após receber o arquivo, arraste-o para a mesma pasta onde o notebook está localizado em seu computador. Isso garante que o caminho relativo na função de leitura permaneça o mesmo e funcione corretamente.\n",
    "   \n",
    "3. **Verifique o Caminho do Arquivo**:\n",
    "   - O código de leitura do arquivo já está implementado no notebook e não precisa ser alterado:\n",
    "     ```python\n",
    "     df = pd.read_csv('BASE DE SINISTRO UNIPAR BRADESCO.csv', decimal=',')\n",
    "     ```\n",
    "   - Certifique-se de que o arquivo CSV esteja no mesmo diretório que o notebook para evitar problemas de caminho.\n",
    "\n",
    "4. **Rodar o Notebook**:\n",
    "   - Com o arquivo posicionado corretamente, execute o notebook normalmente. O pandas irá carregar os dados e você poderá seguir com a análise.\n",
    "\n",
    "**Nota**: Caso o arquivo não esteja na mesma pasta que o notebook, o código não será capaz de localizar o banco de dados, resultando em um erro. Portanto, é essencial que o arquivo seja arrastado para o diretório correto antes da execução.\n",
    "\n",
    "&ensp;Agora que você baixou os notebooks e o banco de dados, pode seguir para os próximos passos de execução, seja localmente ou no Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('BASE DE SINISTRO UNIPAR BRADESCO.csv', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pré-Processamento\n",
    "Como apresentado no notebook 'Pré-Processamento', as linhas a seguir executam várias etapas para limpar e organizar o banco de dados.<br>\n",
    "Os comentários no código explicam cada ação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpeza de dados\n",
    "\n",
    "# Tratamento de valores nulos\n",
    "df = df.dropna()\n",
    "# Correção do ponto faltante em 'UNIPAR INDUPA DO BRASIL S.A'\n",
    "df = df.replace({\"UNIPAR INDUPA DO BRASIL S.A\": \"UNIPAR INDUPA DO BRASIL S.A.\"})\n",
    "# Remoção de AGREGADO e DEPENDENTE\n",
    "df_remove_d = df.loc[(df['Elegibilidade Sinistro'] == 'DEPENDENTE') ]\n",
    "df = df.drop(df_remove_d.index)\n",
    "df_remove_a = df.loc[(df['Elegibilidade Sinistro'] == 'AGREGADO') ]\n",
    "df = df.drop(df_remove_a.index)\n",
    "# Tratamento de valores duplicados\n",
    "df = df.drop_duplicates(keep='last')\n",
    "\n",
    "df.head(10)\n",
    "\n",
    "df_base = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Codificação\n",
    "Este trecho de código realiza a codificação de colunas categóricas, transformando-as em valores numéricos para facilitar o uso no modelo<br>\n",
    "de machine learning. As colunas categóricas são mapeadas para índices numéricos, e o resultado é armazenado em um novo DataFrame.<br>\n",
    "Além disso, a coluna de data é convertida para o formato 'YYYYMMDD', e as colunas numéricas e de data são adicionadas ao DataFrame final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de colunas que serão codificadas (colunas categóricas)\n",
    "colunas_para_codificar = [\n",
    "    'Codigo Empresa Sinistro',\n",
    "    'Sexo Sinistro',\n",
    "    'Faixa-Etária Nova Sinistro',\n",
    "    'Descricao Plano Sinistro',\n",
    "    'Codigo Servico Sinistro',\n",
    "]\n",
    "\n",
    "# Colunas numéricas e de data que não precisam de codificação\n",
    "colunas_nao_codificadas = [\n",
    "    'Dt Data Sinistro',\n",
    "    'Valor Pago Sinistro',\n",
    "]\n",
    "\n",
    "# Para armazenar as novas colunas codificadas\n",
    "novas_colunas_codificadas = {}\n",
    "\n",
    "# Codificação das colunas categóricas\n",
    "for coluna in colunas_para_codificar:\n",
    "        unique_sorted_values = sorted(df[coluna].unique())\n",
    "        df[f'Codificada {coluna}'] = df[coluna].apply(lambda x: unique_sorted_values.index(x))\n",
    "        # Armazenar as novas colunas codificadas\n",
    "        novas_colunas_codificadas[f'Codificada {coluna}'] = df[f'Codificada {coluna}']\n",
    "\n",
    "# Criando um novo DataFrame com as novas colunas codificadas\n",
    "df_novo = pd.DataFrame(novas_colunas_codificadas)\n",
    "\n",
    "# Convertendo a coluna 'Dt Data Sinistro' para o formato 'YYYYMMDD' (Ano-Mês-Dia)\n",
    "df['Dt Data Sinistro'] = pd.to_datetime(df['Dt Data Sinistro'], format='%d/%m/%Y').dt.strftime('%Y%m%d')\n",
    "\n",
    "# Adicionando as colunas numéricas e de data ao DataFrame final\n",
    "for coluna in colunas_nao_codificadas:\n",
    "    df_novo[coluna] = df[coluna]\n",
    "\n",
    "df = df_novo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibindo as 10 primeiras linhas do novo DataFrame\n",
    "df.head(10).sort_values(by='Valor Pago Sinistro', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Detecção e Remoção de Outliers\n",
    "Esse bloco de código remove os outlier da coluna *Valor Pago Sinistro* aplicando o _IsolationForest_ com uma contaminação de 5%, criando um novo DataFrame chamado de `df_clean`. Além disso, ele padroniza as colunas numéricas e as armazena em uma nova variável `numeric_colums` apenas com as colunas numéricas do DataFrame para futuras análises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Aplicar IsolationForest para detectar outliers\n",
    "# iso_forest = IsolationForest(contamination=0.05)  # 5% de contaminação, ajuste conforme necessário\n",
    "# outliers = iso_forest.fit_predict(df[['Valor Pago Sinistro']])\n",
    "\n",
    "# # Remover os outliers\n",
    "# df_clean = df[outliers == 1]\n",
    "\n",
    "# Corrige a chamada de df\n",
    "df_clean = df\n",
    "\n",
    "# Selecionar apenas as colunas numéricas para padronização\n",
    "numeric_columns = df_clean.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Padronizar os dados\n",
    "scaler = StandardScaler()\n",
    "df_clean[numeric_columns] = scaler.fit_transform(df_clean[numeric_columns])\n",
    "\n",
    "# Retorna a notação de df\n",
    "df = df_clean\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.describe().round(2) #Arredonda para duas casas decimais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tratamento de Dados\n",
    "A primeira linha desse bloco converte a coluna **Dt Data Sinistro** para um formato de ano-mês-dia, separando-os em 3 componentes diferentes, porém ainda na mesma coluna.<br>\n",
    "As próximas três linhas separam esses componentes em 3 colunas separadas.<br>\n",
    "Após isso, realizamos a padronização dessas 3 colunas e criamos novas colunas, `Ano_scaled`, `Mes_scaled` e `Dia_scaled`, para armazenar a versão escalonada das colunas originais.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter a coluna para datetime\n",
    "df['Dt Data Sinistro'] = pd.to_datetime(df['Dt Data Sinistro'], format='%Y%m%d')\n",
    "\n",
    "# Extrair componentes de ano, mês e dia\n",
    "df['Ano'] = df['Dt Data Sinistro'].dt.year\n",
    "df['Mes'] = df['Dt Data Sinistro'].dt.month\n",
    "df['Dia'] = df['Dt Data Sinistro'].dt.day\n",
    "\n",
    "# Padronizar (escalonar) os componentes se necessário\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df[['Ano_scaled', 'Mes_scaled', 'Dia_scaled']] = scaler.fit_transform(df[['Ano', 'Mes', 'Dia']])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Remoção de Colunas\n",
    "Este código remove as colunas `Mes` e `Dt Data Sinistro` do DataFrame `df`. A remoção dessas colunas é feita diretamente no DataFrame, utilizando o parâmetro `inplace=True`, o que garante que as alterações sejam aplicadas sem a necessidade de criar uma cópia do DataFrame. O resultado é um DataFrame que não contém mais as colunas especificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Ano', 'Mes', 'Dia'], inplace=True)\n",
    "\n",
    "df.drop(columns=['Dt Data Sinistro'], inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Preparação do Modelo\n",
    "### Valor de K\n",
    "O valor de K no K-means representa o número de clusters que o algoritmo tentará formar, agrupando dados semelhantes.<br>\n",
    "É importante definir K adequadamente, pois ele determina quantos grupos distintos serão gerados no conjunto de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização\n",
    "Antes de calcular o valor de K propriamente dito, buscamos normalizar os dados para garantir que todas as variáveis<br>\n",
    "tenham a mesma escala, evitando que variáveis com valores maiores influenciem mais o agrupamento do que as menores.<br>\n",
    "Para isso, utilizamos a classe ``StandardScaler`` da biblioteca ``scikit-learn`` (sklearn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "dados_normalizado = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Elbow Method* (ou Método do Cotovelo)\n",
    "Com os dados normalizados, o *Elbow Method* ajuda a determinar o número ideal de *clusters* (K) no K-means. Ele traça<br>\n",
    "a inércia (soma das distâncias dos pontos aos seus centróides) para diferentes valores de K. O ponto onde a redução<br>\n",
    "na inércia começa a diminuir significativamente forma um \"cotovelo\" no gráfico, sugerindo o valor ideal de K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(1, 13)\n",
    "wcss = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(df)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(k_values, wcss, 'bo-', markersize=8)\n",
    "plt.xlabel('Número de clusters (k)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.title('Método Elbow para Encontrar o k Ideal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Modelo K-means\n",
    "Após normalizar os dados e identificar o valor ideal de K usando o *Elbow Method* (e o cálculo do Silhouette Score),<br>\n",
    "a próxima etapa é aplicar o K-means. Nessa fase, o algoritmo é inicializado com o valor de K escolhido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialização do modelo\n",
    "O código abaixo divide os dados em treino e teste, normaliza ambos, aplica o K-means com K=7 aos dados de treino e<br>\n",
    "prevê clusters para os dados de teste. Finalmente, cria um DataFrame com os dados de teste e adiciona os rótulos<br>\n",
    " dos clusters atribuídos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_normalizado = scaler.fit_transform(df)\n",
    "\n",
    "k = 7\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "clusters = kmeans.fit_predict(df)\n",
    "df['cluster'] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantidade de Elementos / cluster\n",
    "Verificação de quantos dados estão atribuídos a cada cluster após a aplicação do K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(df['cluster']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados de um cluster específico\n",
    "Verificação de quais dados pertencem a um cluster específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar os dados que pertencem ao cluster 1\n",
    "dados_cluster_1 = df[df['cluster'] == 1]\n",
    "\n",
    "# Verificar os dados do cluster 1\n",
    "dados_cluster_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score\n",
    "Por fim, calculamos o Silhouette Score que mede a qualidade dos clusters, avaliando a coesão interna e a separação<br>\n",
    " entre clusters, com valores próximos de 1 indicando uma boa separação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_avg = silhouette_score(df, df['cluster'])\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de clusters segundo Silhouette Score\n",
    "\n",
    "#### **ATENÇÃO!!!**\n",
    "**O código está comentado devido ao tempo de execução prolongado (pode chegar a horas dependendo do computador).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESABILIDATO POR DEMORAR MUITO\n",
    "\n",
    "# # Lista para armazenar os Silhouette Scores\n",
    "# silhouette_scores = []\n",
    "\n",
    "# # Variando k de 2 a 12\n",
    "# for k in range(2, 13):\n",
    "#     kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "#     kmeans.fit(dados_normalizado)\n",
    "    \n",
    "#     # Prediz os clusters para os dados de teste\n",
    "#     clusters_teste = kmeans.predict(dados_normalizado)\n",
    "    \n",
    "#     # Calcula o Silhouette Score\n",
    "#     score = silhouette_score(dados_normalizado, clusters_teste)\n",
    "#     silhouette_scores.append((k, score))\n",
    "    \n",
    "#     # print(f'k = {k}, Silhouette Score = {score}') # Verificar os resultados\n",
    "\n",
    "# # Converte os resultados para um DataFrame para melhor visualização\n",
    "# silhouette_scores_df = pd.DataFrame(silhouette_scores, columns=['k', 'Silhouette Score'])\n",
    "\n",
    "# # Plotando o gráfico\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(silhouette_scores_df['k'], silhouette_scores_df['Silhouette Score'], marker='o', linestyle='-', color='b')\n",
    "# plt.title('Silhouette Score para Diferentes Valores de k')\n",
    "# plt.xlabel('Número de clusters (k)')\n",
    "# plt.ylabel('Silhouette Score')\n",
    "# plt.xticks(range(2, 13))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de Hiperparâmetros com GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_scorer(estimator, X):\n",
    "    labels = estimator.fit_predict(X)\n",
    "    # Evitar erro se apenas 1 cluster for formado\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        return silhouette_score(X, labels)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Definindo o grid de hiperparâmetros\n",
    "param_grid = {\n",
    "    'n_clusters': [7],\n",
    "    'init': ['k-means++'],\n",
    "    'max_iter': [500]\n",
    "}\n",
    "\n",
    "# # Definindo o grid de hiperparâmetros para testar\n",
    "# param_grid = {\n",
    "#     'n_clusters': [3, 5, 7, 9],\n",
    "#     'init': ['k-means++', 'random'],\n",
    "#     'max_iter': [300, 500, 1000]\n",
    "# }\n",
    "\n",
    "# Configurando o GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=KMeans(random_state=42), \n",
    "    param_grid=param_grid, \n",
    "    scoring=silhouette_scorer, \n",
    "    cv=5,  # Cross-validation\n",
    "    verbose=3,\n",
    ")\n",
    "\n",
    "# Executando o GridSearch\n",
    "grid_search.fit(dados_normalizado)\n",
    "\n",
    "# Exibindo os melhores parâmetros encontrados\n",
    "print(\"Melhores parâmetros: \", grid_search.best_params_)\n",
    "print(\"Melhor silhouette score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização dos clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros ótimos para df: {'init': 'random', 'max_iter': 500, 'n_clusters': 7}\n",
    "X = df.drop('cluster', axis=1)\n",
    "\n",
    "# Visualizando os resultados com os melhores parâmetros\n",
    "best_model = grid_search.best_estimator_\n",
    "df['cluster'] = best_model.predict(X)\n",
    "\n",
    "# Visualizando os clusters\n",
    "scatter = plt.scatter(df['Mes_scaled'],df['Codificada Codigo Servico Sinistro'], c=df['cluster'], cmap='Spectral', edgecolors='black', linewidths=0.2)\n",
    "# plt.scatter(cntr[:, 0], cntr[:, 1], c='red', marker='x')  # Centros dos clusters\n",
    "plt.title('K-Means clustering com Melhores Parâmetros')\n",
    "plt.xlabel('Valor Pago Sinistro')\n",
    "plt.ylabel('Codificada Codigo Servico Sinistro')\n",
    "\n",
    "# Lista de números de clusters\n",
    "clusters = np.unique(df['cluster'])\n",
    "\n",
    "# Criando a legenda com base nos clusters\n",
    "legend_labels = [f'cluster {int(cluster)+1}' for cluster in clusters]\n",
    "\n",
    "# Criando a barra de cores para o gráfico\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(clusters)\n",
    "cbar.set_ticklabels(legend_labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise Gráfica\n",
    "Após a clusterização, essa seção visa analisar graficamente os clusters resultantes do K-means e suas características.<br>\n",
    "Para isso, criamos gráficos associando os clusters e as features que escolhemos de entrada para aplicar o modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribuição de Elementos por cluster\n",
    "Esse gráfico mostra a quantidade de elementos em cada cluster, permitindo visualizar a distribuição e a densidade<br>\n",
    "dos dados entre os clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar o número de observações em cada cluster\n",
    "cluster_counts = df['cluster'].value_counts().sort_index()\n",
    "\n",
    "# Criar um DataFrame para a visualização\n",
    "cluster_counts_df = pd.DataFrame({'cluster': cluster_counts.index, 'Count': cluster_counts.values})\n",
    "\n",
    "# Plotar o gráfico de barras\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='cluster', y='Count', hue='cluster', data=cluster_counts_df, palette='viridis', legend=False)\n",
    "plt.title('Distribuição de Elementos por cluster')\n",
    "plt.xlabel('cluster')\n",
    "plt.ylabel('Número de Observações')\n",
    "plt.xticks(rotation=45)  # Opcional: Rotacionar os rótulos do eixo x se necessário\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relação entre Homens e Mulheres por cluster\n",
    "Esse gráfico mostra a distribuição de gêneros em cada cluster, permitindo visualizar como os grupos se dividem<br>\n",
    "entre homens e mulheres. Testamos a plotagem de dois padrões de gráficos para escolher o que melhor se adequa.<br>\n",
    "O gráfico de barras horizontais foi escolhido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Codificada Sexo Sinistro', hue='cluster', data=df, palette='viridis')\n",
    "plt.title('Relação entre Homens e Mulheres por cluster')\n",
    "plt.xlabel('Gênero')\n",
    "plt.ylabel('Quantidade')\n",
    "plt.xticks(ticks=range(2), labels=['Mulheres', 'Homens'])\n",
    "plt.legend(title='cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiro, calcule as contagens por gênero e cluster\n",
    "contagens = df.groupby(['Codificada Sexo Sinistro', 'cluster']).size().unstack()\n",
    "\n",
    "# Plote o gráfico de barras empilhadas\n",
    "ax = contagens.plot(kind='barh', stacked=True, colormap='viridis', figsize=(10, 6))\n",
    "\n",
    "# Personalize o gráfico\n",
    "plt.title('Relação entre Homens e Mulheres por cluster')\n",
    "plt.xlabel('Quantidade')\n",
    "plt.ylabel('Gênero')\n",
    "plt.yticks(ticks=range(2), labels=['Mulheres', 'Homens'])\n",
    "plt.legend(title='cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relação entre Faixas Etárias por cluster\n",
    "Esse gráfico mostra a distribuição das faixas etárias dentro de cada cluster, permitindo visualizar como diferentes<br>\n",
    "grupos de idade estão distribuídos entre os clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar a quantidade de ocorrências para cada combinação de faixa etária e cluster\n",
    "contagem_df = df.groupby(['Codificada Faixa-Etária Nova Sinistro', 'cluster']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plotar gráfico de barras acumuladas\n",
    "plt.figure(figsize=(10, 6))\n",
    "contagem_df.plot(kind='bar', stacked=True, colormap='viridis', edgecolor='none')\n",
    "plt.title('Relação entre Faixas Etárias por cluster')\n",
    "plt.xlabel('Faixa Etária')\n",
    "plt.ylabel('Quantidade')\n",
    "plt.xticks(ticks=range(len(contagem_df.index)), labels=['0-18','19-23','24-28','29-33','34-38','39-43','44-48','49-53','54-58','58 ou mais'], rotation=45)\n",
    "plt.legend(title='cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relação entre Valor Pago por cluster Mensalmente\n",
    "Esse gráfico mostra a distribuição do valor pago por mês para cada cluster, permitindo comparar os gastos médios<br>\n",
    "ou totais entre os diferentes grupos formados pelo K-means.<br>\n",
    "(Ainda estamos analisando a viabilidade desse gráfico para o projeto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratar_outliers_por_cluster(df, coluna):\n",
    "    # DataFrame para armazenar dados limpos\n",
    "    df_limpo = pd.DataFrame()\n",
    "\n",
    "    for cluster in df['cluster'].unique():\n",
    "        # Selecionar dados para o cluster\n",
    "        dados_cluster = df[df['cluster'] == cluster]\n",
    "        \n",
    "        # Calcular o IQR\n",
    "        Q1 = dados_cluster[coluna].quantile(0.25)\n",
    "        Q3 = dados_cluster[coluna].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Definir limites para outliers\n",
    "        limite_inferior = Q1 - 1.5 * IQR\n",
    "        limite_superior = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Filtrar dados sem outliers\n",
    "        dados_limpos = dados_cluster[(dados_cluster[coluna] >= limite_inferior) & (dados_cluster[coluna] <= limite_superior)]\n",
    "        \n",
    "        # Adicionar dados limpos ao DataFrame final\n",
    "        df_limpo = pd.concat([df_limpo, dados_limpos])\n",
    "    \n",
    "    return df_limpo\n",
    "\n",
    "# Tratar outliers\n",
    "dados_normalizado = tratar_outliers_por_cluster(df, 'Valor Pago Sinistro')\n",
    "\n",
    "# Criar gráfico de caixa\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=dados_normalizado, x='cluster', y='Valor Pago Sinistro', hue='cluster', palette='viridis', legend=False)\n",
    "\n",
    "# Adicionar título e rótulos aos eixos\n",
    "plt.title('Distribuição do Valor Pago Sinistro por cluster (Sem Outliers)')\n",
    "plt.xlabel('cluster')\n",
    "plt.ylabel('Valor Pago Sinistro')\n",
    "\n",
    "# Exibir o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identificação dos dados de um cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1_data = df[df['cluster'] == 1]\n",
    "\n",
    "cluster_1_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifica a correlação entre _features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PCA\n",
    "O método PCA é uma técnica de redução de dimensionalidade usada para simplificar grandes conjuntos de dados,<br>\n",
    "preservando o máximo de variância possível. Utilizamos ele para obter uma melhor visualização dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2d = PCA(n_components=2)\n",
    "dados_treino_pca = pca_2d.fit_transform(df)\n",
    "dados_teste_pca = pca_2d.transform(df)\n",
    "dados_teste_pca_df = pd.DataFrame(dados_teste_pca, columns=['PC1', 'PC2'])\n",
    "dados_teste_pca_df['cluster'] = df['cluster']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=dados_teste_pca_df, x='PC1', y='PC2', hue=df['cluster'], palette='viridis', s=60)\n",
    "plt.title('clusters previstos pelo K-Means (2D PCA)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_3d = PCA(n_components=3)\n",
    "dados_teste_pca = pca_3d.fit_transform(df)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(dados_teste_pca[:, 0], dados_teste_pca[:, 1], dados_teste_pca[:, 2], c=df['cluster'], cmap='viridis')\n",
    "\n",
    "ax.set_title(\"clusters previstos pelo K-Means (3D PCA)\")\n",
    "ax.set_xlabel(\"Componente Principal 1\")\n",
    "ax.set_ylabel(\"Componente Principal 2\")\n",
    "ax.set_zlabel(\"Componente Principal 3\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo de métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando o modelo K-Means com 7 clusters\n",
    "kmeans = KMeans(n_clusters=7, init='k-means++', max_iter=300, random_state=42)\n",
    "kmeans.fit(df)\n",
    "\n",
    "# Adicionando rótulos ao DataFrame\n",
    "df['cluster'] = kmeans.labels_\n",
    "\n",
    "# Calculando o Silhouette Score\n",
    "silhouette_avg = silhouette_score(df, df['cluster'])\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "# Calculando o Davies-Bouldin Index\n",
    "dbi = davies_bouldin_score(df, df['cluster'])\n",
    "print(f\"Davies-Bouldin Index: {dbi}\")\n",
    "\n",
    "# Calculando o Calinski-Harabasz Index\n",
    "chi = calinski_harabasz_score(df, df['cluster'])\n",
    "print(f\"Calinski-Harabasz Index: {chi}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
