{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aviso\n",
    "Não conseguimos refinar esse notebook para a entrega da sprint 3 e está planejado para ser analisado<br>\n",
    "se realmente vamos realizar a análise preditiva dividida por níveis de valor de sinistro.<br>\n",
    "O grupo decidirá na planning da sprint 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.datasets import make_blobs #???\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "df = pd.read_csv('df_completo_codificado(10).csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checagem dos _imports_ do banco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação por níveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faixas_valor = [0, 50, 100, 200, 500, 10000, float(\"inf\")]\n",
    "\n",
    "categorias_valor = [1,2,3,4,5,6]\n",
    "\n",
    "df[\"Categoria Sinistro\"] = pd.cut(df[\"Valor Pago Sinistro\"], bins=faixas_valor, labels=categorias_valor)\n",
    "\n",
    "df['Categoria Sinistro'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificação dos valores nos níveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir variáveis diretamente\n",
    "for nivel in range(1, 7):\n",
    "    exec(f'df_nivel{nivel} = df.loc[df[\"Categoria Sinistro\"] == {nivel}]')\n",
    "\n",
    "df_nivel1.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot do _Elbow Method_ para cada nível"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo do k ótimo (Elbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonte: https://medium.com/pizzadedados/kmeans-e-metodo-do-cotovelo-94ded9fdf3a9\n",
    "\n",
    "x1, y1 = 2, wcss[0]\n",
    "x2, y2 = 20, wcss[len(wcss)-1]\n",
    "\n",
    "distances = []\n",
    "for i in range(len(wcss)):\n",
    "    x0 = i+2\n",
    "    y0 = wcss[i]\n",
    "    numerator = abs((y2-y1)*x0 - (x2-x1)*y0 + x2*y1 - y2*x1)\n",
    "    denominator = np.sqrt((y2 - y1)**2 + (x2 - x1)**2)\n",
    "    distances.append(numerator/denominator)\n",
    "\n",
    "distances.index(max(distances)) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o intervalo de k_values\n",
    "k_values = range(1, 9)\n",
    "\n",
    "# Criar a figura e os subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18, 10))  # Ajuste o tamanho da figura conforme necessário\n",
    "fig.suptitle('Método Elbow para Encontrar o k Ideal por Nível', fontsize=20, fontweight='bold')\n",
    "\n",
    "# Iterar sobre cada nível e plotar em subplots\n",
    "for nivel in range(1, 7):\n",
    "    # Filtrar o DataFrame para o nível atual\n",
    "    df_nivel = df.loc[df['Categoria Sinistro'] == nivel]\n",
    "    \n",
    "    wcss = []\n",
    "\n",
    "    # Calcular WCSS para cada valor de k\n",
    "    for k in k_values:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(df_nivel)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "    # Encontrar o k ótimo usando o método da distância\n",
    "    x1, y1 = 2, wcss[0]\n",
    "    x2, y2 = 8, wcss[-1]  # 8 é o último valor de k\n",
    "    \n",
    "    distances = []\n",
    "    for i in range(len(wcss)):\n",
    "        x0 = i + 1  # k_values começa em 1, mas o índice começa em 0\n",
    "        y0 = wcss[i]\n",
    "        numerator = abs((y2 - y1) * x0 - (x2 - x1) * y0 + x2 * y1 - y2 * x1)\n",
    "        denominator = np.sqrt((y2 - y1) ** 2 + (x2 - x1) ** 2)\n",
    "        distances.append(numerator / denominator)\n",
    "    \n",
    "    k_otimo = distances.index(max(distances)) + 1  # Adiciona 1 porque o índice começa em 0\n",
    "\n",
    "    # Escolher o subplot para o nível atual\n",
    "    ax = axs[(nivel - 1) // 3, (nivel - 1) % 3]\n",
    "\n",
    "    # Escolher uma paleta de cores\n",
    "    viridis_palette = sns.color_palette(\"viridis\", n_colors=6)\n",
    "    \n",
    "    # Plotar os dados com Seaborn\n",
    "    sns.lineplot(x=k_values, y=wcss, marker='o', linewidth=2, color=viridis_palette[nivel - 1], ax=ax)\n",
    "\n",
    "    \n",
    "    # Destacar o k ótimo no gráfico\n",
    "    ax.plot(k_otimo, wcss[k_otimo - 1], 'ro', markersize=10)  # Ponto ótimo em vermelho\n",
    "    ax.annotate(f'k={k_otimo}', (k_otimo, wcss[k_otimo - 1]), textcoords=\"offset points\", xytext=(10,15), ha='center', fontsize=12, color='red')\n",
    "    \n",
    "    # Configurar os rótulos e título\n",
    "    ax.set_xlabel('Número de Clusters (k)', fontsize=12)\n",
    "    ax.set_ylabel('WCSS', fontsize=12)\n",
    "    ax.set_title(f'Nível {nivel}', fontsize=14)\n",
    "    \n",
    "    # Remover a grade\n",
    "    ax.grid(False)\n",
    "\n",
    "    # Print do valor ótimo de k\n",
    "    print(f'Nível {nivel}: k = {k_otimo}')\n",
    "\n",
    "# Ajustar o layout para que não haja sobreposição\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Ajusta o título da figura\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o limiar de variância para a seleção de características\n",
    "var_thr = VarianceThreshold(threshold=0.25)  # Ajusta o threshold conforme necessário\n",
    "\n",
    "# Ajustar o VarianceThreshold aos dados\n",
    "var_thr.fit(df)\n",
    "\n",
    "# Obter o suporte (máscara booleana) para as características selecionadas\n",
    "support = var_thr.get_support()\n",
    "\n",
    "# Identificar as colunas constantes e quase constantes\n",
    "con_cols = [col for col in df.columns if col not in df.columns[support]]\n",
    "\n",
    "# Remover as colunas constantes e quase constantes do DataFrame\n",
    "df_reduzido = df.drop(con_cols, axis=1)\n",
    "\n",
    "# Exibir as colunas removidas e o DataFrame reduzido\n",
    "print(\"Colunas removidas:\", con_cols)\n",
    "print(\"DataFrame reduzido:\")\n",
    "print(df_reduzido.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajustar o scaler aos dados e transformar os dados\n",
    "scaled_data = pd.DataFrame(scaler.fit_transform(df_reduzido), columns=df_reduzido.columns)\n",
    "\n",
    "# Exibir as primeiras linhas dos dados normalizados\n",
    "print(\"Dados normalizados:\")\n",
    "print(scaled_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o PCA com o número de componentes igual ao número de características normalizadas\n",
    "pca = PCA(n_components=scaled_data.shape[1])\n",
    "\n",
    "# Ajustar o PCA aos dados normalizados\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "# Calcular a variância explicada e a variância explicada acumulada\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Função para plotar a variância explicada\n",
    "def pca_plot(cumulative_explained_variance, data): # data ???\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
    "    plt.xlabel('Número de Componentes Principais')\n",
    "    plt.ylabel('Variância Explicada Acumulada')\n",
    "    plt.title('Variância Explicada Acumulada com PCA')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar a variância explicada acumulada\n",
    "pca_plot(cumulative_explained_variance, scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA para melhor número de colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Definir o limiar de variância desejada\n",
    "# desired_explained_variance = 0.95\n",
    "\n",
    "# # Selecionar o número de componentes com base no limiar de variância explicada\n",
    "# num_components = np.argmax(cumulative_explained_variance >= desired_explained_variance) + 1\n",
    "\n",
    "# # Exibir o número selecionado de componentes\n",
    "# print(f\"Número de componentes para explicar {desired_explained_variance:.2f} da variância: {num_components}\")\n",
    "\n",
    "# # Ajustar o PCA com o número selecionado de componentes\n",
    "# pca = PCA(n_components=num_components)\n",
    "# PCA_ds = pd.DataFrame(pca.fit_transform(scaled_data), columns=[f\"PC{i+1}\" for i in range(num_components)])\n",
    "\n",
    "# # Exibir as primeiras linhas do DataFrame com os componentes principais\n",
    "# print(\"DataFrame com os componentes principais:\")\n",
    "# print(PCA_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA para duas colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar o PCA com o número selecionado de componentes\n",
    "pca = PCA(n_components=2)\n",
    "PCA_ds = pd.DataFrame(pca.fit_transform(scaled_data))\n",
    "\n",
    "# Exibir as primeiras linhas do DataFrame com os componentes principais\n",
    "print(\"DataFrame com os componentes principais:\")\n",
    "print(PCA_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando pelo Método Silhouette "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o intervalo de valores de K para explorar\n",
    "k_values = range(2, 10)\n",
    "silhouette_scores = []\n",
    "\n",
    "best_k = None\n",
    "best_silhouette_score = -1\n",
    "\n",
    "# Iterar através dos diferentes valores de K\n",
    "for k in k_values:\n",
    "    km = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=100, random_state=42)\n",
    "    km.fit(PCA_ds)\n",
    "    \n",
    "    # Calcular a média da pontuação de Silhouette\n",
    "    silhouette_avg = silhouette_score(PCA_ds, km.labels_)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    \n",
    "    # Atualizar o melhor valor de K se uma pontuação de Silhouette mais alta for encontrada\n",
    "    if silhouette_avg > best_silhouette_score:\n",
    "        best_k = k\n",
    "        best_silhouette_score = silhouette_avg\n",
    "\n",
    "# Exibir o melhor valor de K e a pontuação de Silhouette correspondente\n",
    "print(f\"Melhor valor de K: {best_k}\")\n",
    "print(f\"Melhor Pontuação de Silhouette: {best_silhouette_score:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, silhouette_scores, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Número de Clusters (K)')\n",
    "plt.ylabel('Pontuação de Silhouette')\n",
    "plt.title('Pontuação de Silhouette para Diferentes Valores de K')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o intervalo de k_values\n",
    "k_values = range(2, 8)  # Começa em 2 porque o Silhouette Score não é definido para k=1\n",
    "\n",
    "# Criar a figura e os subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18, 10))  # Ajuste o tamanho da figura conforme necessário\n",
    "fig.suptitle('Método Silhouette para Encontrar o k Ideal por Nível', fontsize=16)\n",
    "\n",
    "# Definir a paleta de cores\n",
    "viridis_palette = sns.color_palette(\"viridis\", n_colors=6)\n",
    "\n",
    "# Iterar sobre cada nível e plotar em subplots\n",
    "for nivel in range(1, 7):\n",
    "    # Filtrar o DataFrame dentro do nível 1\n",
    "    df_nivel = df.loc[df['Categoria Sinistro'] == nivel]\n",
    "    \n",
    "    silhouette_scores = []\n",
    "\n",
    "    # Calcular o Silhouette Score para cada valor de k\n",
    "    for k in k_values:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(df_nivel)\n",
    "        silhouette_avg = silhouette_score(df_nivel, cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    # Escolher o subplot para o nível atual\n",
    "    ax = axs[(nivel - 1) // 3, (nivel - 1) % 3]\n",
    "    sns.lineplot(x=k_values, y=silhouette_scores, marker='o', linewidth=2, color=viridis_palette[nivel - 1], ax=ax)\n",
    "    ax.set_xlabel('Número de Clusters (k)')\n",
    "    ax.set_ylabel('Silhouette Score')\n",
    "    ax.set_title(f'Nível {nivel}')\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Ajustar o layout para que não haja sobreposição\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Ajusta o título da figura\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o intervalo de k_values\n",
    "k_values = range(2, 8)  # Começa em 2 porque o Silhouette Score não é definido para k=1\n",
    "\n",
    "# Criar a figura e os subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18, 10))  # Ajuste o tamanho da figura conforme necessário\n",
    "fig.suptitle('Método Silhouette para Encontrar o k Ideal por Nível', fontsize=16)\n",
    "\n",
    "# Iterar sobre cada nível e plotar em subplots\n",
    "for nivel in range(1, 7):\n",
    "    # Filtrar o DataFrame para o nível atual\n",
    "    df_nivel = df.loc[df['Categoria Sinistro'] == nivel].copy()\n",
    "    \n",
    "    silhouette_scores = []\n",
    "\n",
    "    # Calcular o Silhouette Score para cada valor de k\n",
    "    for k in k_values:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(df_nivel)\n",
    "        silhouette_avg = silhouette_score(df_nivel, cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    # Escolher uma paleta de cores\n",
    "    viridis_palette = sns.color_palette(\"viridis\", n_colors=6)\n",
    "\n",
    "    # Escolher o subplot para o nível atual\n",
    "    ax = axs[(nivel - 1) // 3, (nivel - 1) % 3]\n",
    "    sns.lineplot(x=k_values, y=silhouette_scores, marker='o', linewidth=2, color=viridis_palette[nivel - 1], ax=ax)\n",
    "    ax.set_xlabel('Número de Clusters (k)')\n",
    "    ax.set_ylabel('Silhouette Score')\n",
    "    ax.set_title(f'Nível {nivel}')\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Ajustar o layout para que não haja sobreposição\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Ajusta o título da figura\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values\n",
    "\n",
    "range_n_clusters = [6, 7, 8, 9, 10]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator seed of 10 for reproducibility\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    centers = clusterer.cluster_centers_\n",
    "    ax2.scatter(\n",
    "        centers[:, 0],\n",
    "        centers[:, 1],\n",
    "        marker=\"o\",\n",
    "        c=\"white\",\n",
    "        alpha=1,\n",
    "        s=200,\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\" % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o intervalo de k_values e a porcentagem de dados de teste\n",
    "k_values = range(2, 9)  # k variando de 3 até 8\n",
    "test_size = 0.3\n",
    "random_state = 42\n",
    "\n",
    "# Iterar sobre os níveis de 'Categoria Sinistro'\n",
    "for nivel in range(1, 7):\n",
    "    # Filtrar o DataFrame para o nível atual\n",
    "    df_nivel = df.loc[df['Categoria Sinistro'] == nivel]\n",
    "    \n",
    "    # Dividir os dados entre treino e teste\n",
    "    dados_treino, dados_teste = train_test_split(df_nivel, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Inicializar o StandardScaler e normalizar os dados de treino e teste\n",
    "    scaler = StandardScaler()\n",
    "    dados_treino_normalizado = scaler.fit_transform(dados_treino)\n",
    "    dados_teste_normalizado = scaler.transform(dados_teste)\n",
    "\n",
    "    # Iterar sobre os diferentes valores de k\n",
    "    for k in k_values:\n",
    "        # Aplicar o K-Means no conjunto de treino\n",
    "        kmeans = KMeans(n_clusters=k, random_state=random_state)\n",
    "        kmeans.fit(dados_treino_normalizado)\n",
    "        \n",
    "        # Prever os clusters para o conjunto de teste\n",
    "        clusters_teste = kmeans.predict(dados_teste_normalizado)\n",
    "        \n",
    "        # Calcular o Silhouette Score para o conjunto de teste\n",
    "        silhouette_avg = silhouette_score(dados_teste_normalizado, clusters_teste)\n",
    "        \n",
    "        # Imprimir o Silhouette Score para o nível atual e valor de k\n",
    "        print(f\"Nível {nivel} - k={k} - Silhouette Score para os dados de teste: {silhouette_avg:.4f}\")\n",
    "    print(\"-\" * 50)  # Separador para cada nível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a porcentagem de dados de teste e outros parâmetros\n",
    "test_size = 0.3\n",
    "random_state = 42\n",
    "k = 5  # Pode ser ajustado para o valor de k desejado\n",
    "\n",
    "# Criar a figura com subplots para os 6 níveis\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(18, 12))  # 2 linhas e 3 colunas de subplots\n",
    "fig.suptitle(\"Clusters previstos pelo K-Means para cada nível\", fontsize=16)\n",
    "\n",
    "# Iterar sobre os níveis de 'Categoria Sinistro'\n",
    "for nivel in range(1, 7):\n",
    "    # Filtrar o DataFrame para o nível atual\n",
    "    df_nivel = df.loc[df['Categoria Sinistro'] == nivel]\n",
    "    \n",
    "    # Dividir os dados entre treino e teste\n",
    "    dados_treino, dados_teste = train_test_split(df_nivel, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Normalizar os dados de treino e teste\n",
    "    scaler = StandardScaler()\n",
    "    dados_treino_normalizado = scaler.fit_transform(dados_treino)\n",
    "    dados_teste_normalizado = scaler.transform(dados_teste)\n",
    "    \n",
    "    # Aplicar o K-Means no conjunto de treino\n",
    "    kmeans = KMeans(n_clusters=k, random_state=random_state)\n",
    "    kmeans.fit(dados_treino_normalizado)\n",
    "    \n",
    "    # Prever os clusters para o conjunto de teste\n",
    "    clusters_teste = kmeans.predict(dados_teste_normalizado)\n",
    "    \n",
    "    # Aplicar PCA para reduzir a dimensionalidade para 2 componentes\n",
    "    pca = PCA(n_components=2)\n",
    "    dados_teste_pca = pca.fit_transform(dados_teste_normalizado)\n",
    "    \n",
    "    # Selecionar o subplot correspondente ao nível atual\n",
    "    ax = axs[(nivel - 1) // 3, (nivel - 1) % 3]\n",
    "    \n",
    "    # Plotar os clusters no espaço PCA\n",
    "    scatter = ax.scatter(dados_teste_pca[:, 0], dados_teste_pca[:, 1], c=clusters_teste, cmap='viridis', s=50)\n",
    "    \n",
    "    ax.set_title(f\"Nível {nivel}\")\n",
    "    ax.set_xlabel(\"Componente Principal 1\")\n",
    "    ax.set_ylabel(\"Componente Principal 2\")\n",
    "\n",
    "# Ajustar o layout para que os gráficos não se sobreponham\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# Adicionar uma barra de cores para os clusters\n",
    "cbar = fig.colorbar(scatter, ax=axs, orientation='vertical', fraction=0.02, pad=0.02)\n",
    "cbar.set_label('Clusters')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dados_teste_pca[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a porcentagem de dados de teste e o random_state\n",
    "test_size = 0.3\n",
    "random_state = 42\n",
    "\n",
    "# Iterar sobre os níveis de 'Categoria Sinistro'\n",
    "for nivel in range(1, 7):\n",
    "    # Filtrar o DataFrame para o nível atual\n",
    "    df_nivel = df.loc[df['Categoria Sinistro'] == nivel]\n",
    "    \n",
    "    # Dividir os dados entre treino e teste\n",
    "    dados_treino, dados_teste = train_test_split(df_nivel, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Inicializar o StandardScaler e normalizar os dados de treino e teste\n",
    "    scaler = StandardScaler()\n",
    "    dados_treino_normalizado = scaler.fit_transform(dados_treino)\n",
    "    dados_teste_normalizado = scaler.transform(dados_teste)\n",
    "    \n",
    "    # Aplicar K-Means\n",
    "    k = 5  # Definir o número de clusters (ou variar conforme desejado)\n",
    "    kmeans = KMeans(n_clusters=k, random_state=random_state)\n",
    "    kmeans.fit(dados_treino_normalizado)\n",
    "    clusters_teste = kmeans.predict(dados_teste_normalizado)\n",
    "    \n",
    "    # Aplicar PCA para reduzir para 3 componentes\n",
    "    pca = PCA(n_components=3)\n",
    "    dados_teste_pca = pca.fit_transform(dados_teste_normalizado)\n",
    "    \n",
    "    # Plotar os clusters em 3D com a projeção do PCA\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Scatter plot com os clusters usando a paleta Viridis\n",
    "    scatter = ax.scatter(dados_teste_pca[:, 0], dados_teste_pca[:, 1], dados_teste_pca[:, 2], \n",
    "                         c=clusters_teste, cmap='viridis', s=50, alpha=0.8)\n",
    "    \n",
    "    ax.set_title(f\"Clusters previstos pelo K-Means (3D PCA) - Nível {nivel}\")\n",
    "    ax.set_xlabel(\"Componente Principal 1\")\n",
    "    ax.set_ylabel(\"Componente Principal 2\")\n",
    "    ax.set_zlabel(\"Componente Principal 3\")\n",
    "    \n",
    "    # Adicionar legenda para os clusters\n",
    "    legend = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "    ax.add_artist(legend)\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
