{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusterização (Modelo Não Supervisionado DBSCAN)\n",
    "\n",
    "Este notebook explora o algoritmo de aprendizado de máquina não supervisionado DBSCAN (Density-Based Spatial Clustering of Applications with Noise). <br> O objetivo deste método é identificar clusters de alta densidade em um conjunto de dados, enquanto separa os pontos que são considerados ruído. Através dessa abordagem, o DBSCAN é capaz de detectar formas arbitrárias de clusters, permitindo uma análise mais robusta das estruturas subjacentes nos dados.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Instalação de bibliotecas\n",
    "Instala bibliotecas necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas==2.1.4 numpy==1.26.4 matplotlib==3.7.5 seaborn==0.13.2 scikit-learn==1.4.2 plotly==5.24.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Inicialização de bibliotecas\n",
    "Importa as bibliotecas necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, silhouette_samples\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # Mostra todas as colunas\n",
    "\n",
    "# Configuração para exibir os gráficos diretamente no notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Importe do banco de dados via arquivo local\n",
    "Importação do DataFrame da base de dados da Unipar com a biblioteca pandas:\n",
    "\n",
    "Para rodar o notebook corretamente, será necessário importar o arquivo de banco de dados manualmente, pois ele não está incluído no repositório devido à presença de dados sensíveis.\n",
    "\n",
    "#### Passos para Importar o Banco de Dados no VS Code\n",
    "\n",
    "1. **Obtenha o Arquivo de Dados**:\n",
    "   - O arquivo `BASE DE SINISTRO UNIPAR BRADESCO.csv` deverá ser fornecido separadamente. Entre em contato com o responsável pelo projeto para receber o arquivo.\n",
    "\n",
    "2. **Posicione o Arquivo na Pasta Correta**:\n",
    "   - Após receber o arquivo, arraste-o para a mesma pasta onde o notebook está localizado em seu computador. Isso garante que o caminho relativo na função de leitura permaneça o mesmo e funcione corretamente.\n",
    "   \n",
    "3. **Verifique o Caminho do Arquivo**:\n",
    "   - O código de leitura do arquivo já está implementado no notebook e não precisa ser alterado:\n",
    "     ```python\n",
    "     df = pd.read_csv('BASE DE SINISTRO UNIPAR BRADESCO.csv', decimal=',')\n",
    "     ```\n",
    "   - Certifique-se de que o arquivo CSV esteja no mesmo diretório que o notebook para evitar problemas de caminho.\n",
    "\n",
    "4. **Rodar o Notebook**:\n",
    "   - Com o arquivo posicionado corretamente, execute o notebook normalmente. O pandas irá carregar os dados e você poderá seguir com a análise.\n",
    "\n",
    "**Nota**: Caso o arquivo não esteja na mesma pasta que o notebook, o código não será capaz de localizar o banco de dados, resultando em um erro. Portanto, é essencial que o arquivo seja arrastado para o diretório correto antes da execução.\n",
    "\n",
    "&ensp;Agora que você baixou os notebooks e o banco de dados, pode seguir para os próximos passos de execução, seja localmente ou no Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('BASE DE SINISTRO UNIPAR BRADESCO.csv', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pré-Processamento\n",
    "Como apresentado no notebook 'Pré-Processamento', as linhas a seguir executam várias etapas para limpar e organizar o banco de dados.<br>\n",
    "Os comentários no código explicam cada ação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpeza de dados\n",
    "\n",
    "# Tratamento de valores nulos\n",
    "df = df.dropna()\n",
    "# Correção do ponto faltante em 'UNIPAR INDUPA DO BRASIL S.A'\n",
    "df = df.replace({\"UNIPAR INDUPA DO BRASIL S.A\": \"UNIPAR INDUPA DO BRASIL S.A.\"})\n",
    "# Remoção de AGREGADO e DEPENDENTE\n",
    "df_remove_d = df.loc[(df['Elegibilidade Sinistro'] == 'DEPENDENTE') ]\n",
    "df = df.drop(df_remove_d.index)\n",
    "df_remove_a = df.loc[(df['Elegibilidade Sinistro'] == 'AGREGADO') ]\n",
    "df = df.drop(df_remove_a.index)\n",
    "# Tratamento de valores duplicados\n",
    "df = df.drop_duplicates(keep='last')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Codificação\n",
    "Este trecho de código realiza a codificação de colunas categóricas, transformando-as em valores numéricos para facilitar o uso no modelo<br>\n",
    "de machine learning. As colunas categóricas são mapeadas para índices numéricos, e o resultado é armazenado em um novo DataFrame.<br>\n",
    "Além disso, a coluna de data é convertida para o formato 'YYYYMMDD', e as colunas numéricas e de data são adicionadas ao DataFrame final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de colunas que serão codificadas (colunas categóricas)\n",
    "colunas_para_codificar = [\n",
    "    'Codigo Empresa Sinistro',\n",
    "    'Sexo Sinistro',\n",
    "    'Faixa-Etária Nova Sinistro',\n",
    "    'Descricao Plano Sinistro',\n",
    "    'Codigo Servico Sinistro',\n",
    "]\n",
    "\n",
    "# Colunas numéricas e de data que não precisam de codificação\n",
    "colunas_nao_codificadas = [\n",
    "    'Dt Data Sinistro',\n",
    "    'Valor Pago Sinistro',\n",
    "]\n",
    "\n",
    "# Para armazenar as novas colunas codificadas\n",
    "novas_colunas_codificadas = {}\n",
    "\n",
    "# Codificação das colunas categóricas\n",
    "for coluna in colunas_para_codificar:\n",
    "        unique_sorted_values = sorted(df[coluna].unique())\n",
    "        df[f'{coluna}'] = df[coluna].apply(lambda x: unique_sorted_values.index(x))\n",
    "        # Armazenar as novas colunas codificadas\n",
    "        novas_colunas_codificadas[f'{coluna}'] = df[f'{coluna}']\n",
    "\n",
    "# Criando um novo DataFrame com as novas colunas codificadas\n",
    "df_novo = pd.DataFrame(novas_colunas_codificadas)\n",
    "\n",
    "# Convertendo a coluna 'Dt Data Sinistro' para o formato 'YYYYMMDD' (Ano-Mês-Dia)\n",
    "df['Dt Data Sinistro'] = pd.to_datetime(df['Dt Data Sinistro'], format='%d/%m/%Y').dt.strftime('%Y%m%d')\n",
    "\n",
    "# Adicionando as colunas numéricas e de data ao DataFrame final\n",
    "for coluna in colunas_nao_codificadas:\n",
    "    df_novo[coluna] = df[coluna]\n",
    "\n",
    "df = df_novo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibindo as 10 primeiras linhas do novo DataFrame\n",
    "df.head(10).sort_values(by='Valor Pago Sinistro', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Detecção e Remoção de Outliers\n",
    "Esse bloco de código remove os outlier da coluna *Valor Pago Sinistro* aplicando o _IsolationForest_ com uma contaminação de 5%, criando um novo DataFrame chamado de `df_clean`. Além disso, ele padroniza as colunas numéricas e as armazena em uma nova variável `numeric_colums` apenas com as colunas numéricas do DataFrame para futuras análises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar IsolationForest apenas na coluna 'Valor Pago Sinistro'\n",
    "iso_forest = IsolationForest(contamination=0.05)  # Ajuste a contaminação conforme necessário\n",
    "outliers = iso_forest.fit_predict(df[['Valor Pago Sinistro']])\n",
    "\n",
    "# Remover os outliers apenas da coluna 'Valor Pago Sinistro'\n",
    "df_clean = df[outliers == 1]\n",
    "\n",
    "# Selecionar apenas as colunas numéricas para padronização\n",
    "numeric_columns = df_clean.select_dtypes(include=['float64', 'int64']).columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.describe().round(2) #Arredonda para duas casas decimais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tratamento de Dados\n",
    "\n",
    "Este código padroniza as colunas numéricas no DataFrame `df_clean` após a remoção de outliers, utilizando o `StandardScaler` para ajustar os dados à média 0 e desvio padrão 1. \n",
    "\n",
    "A função `fit_transform` é aplicada para transformar as colunas numéricas, e o resultado é armazenado diretamente no DataFrame. Por fim, as primeiras linhas do DataFrame limpo e padronizado são exibidas com `df_clean.head()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padronizar os dados após remover os outliers\n",
    "scaler = StandardScaler()\n",
    "df_clean.loc[:, numeric_columns] = scaler.fit_transform(df_clean[numeric_columns])  # Usando .loc para evitar o SettingWithCopyWarning\n",
    "\n",
    "# Mostrar as primeiras linhas do dataframe limpo e padronizado\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10).sort_values(by='Valor Pago Sinistro', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conversão e Extração de Componentes de Data\n",
    "\n",
    "Este código converte a coluna `Dt Data Sinistro` do DataFrame `df` para o formato datetime, utilizando o padrão 'YYYYMMDD'. Em seguida, extrai o componente do mês da data e o armazena em uma nova coluna chamada `Mes`. Se necessário, os valores dessa coluna são padronizados utilizando o `StandardScaler`, com o resultado armazenado na nova coluna `Mes_scaled`. O DataFrame resultante contém as colunas originais e as novas colunas derivadas da data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter a coluna para datetime\n",
    "df['Dt Data Sinistro'] = pd.to_datetime(df['Dt Data Sinistro'], format='%Y%m%d')\n",
    "\n",
    "# Extrair componentes de dia\n",
    "df['Mes'] = df['Dt Data Sinistro'].dt.month\n",
    "\n",
    "# Padronizar (escalonar) os componentes se necessário\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df[[\"Mes_scaled\"]] = scaler.fit_transform(df[[\"Mes\"]])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Remoção de Colunas\n",
    "Este código remove as colunas `Mes` e `Dt Data Sinistro` do DataFrame `df`. A remoção dessas colunas é feita diretamente no DataFrame, utilizando o parâmetro `inplace=True`, o que garante que as alterações sejam aplicadas sem a necessidade de criar uma cópia do DataFrame. O resultado é um DataFrame que não contém mais as colunas especificadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Mes'], inplace=True)\n",
    "\n",
    "df.drop(columns=['Dt Data Sinistro'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exploração de dados\n",
    "### Matriz de Disperção e Correlação\n",
    "Esse bloco gera uma matriz de gráficos de disperção (*scatter plots*) entre cada par de variáveis numéricas presentes no `df_clean`.<br>\n",
    "Usamos o pairplot para poder ter uma visualização mais rápida e intuitiva das relações entre as variáveis, facilitando a identificação de correlações que fazem mais sentido.\n",
    "\n",
    "A segunda célula calcula uma matriz de correlação para identificar o grau de influência entre duas features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Habilitar de acordo com a necessidade removendo o símbolo: #\n",
    "\n",
    "# Verificar a relação entre as variáveis\n",
    "# sns.pairplot(df_clean) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Habilitar de acordo com a necessidade removendo o símbolo: #\n",
    "\n",
    "# Calcular a matriz de correlação\n",
    "# sns.heatmap(df_clean.corr(), annot=True, cmap=\"coolwarm\", linewidths=0.5) \n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Modelo DBSCAN\n",
    "Este código executa o algoritmo de clustering DBSCAN utilizando os parâmetros previamente definidos, com `eps` igual a 7 e `min_samples` igual a 2. Após a execução do algoritmo, os rótulos resultantes são obtidos e os ruídos, representados por -1, são excluídos para calcular o número de clusters.\n",
    "\n",
    "Se mais de um cluster for encontrado, é calculada a métrica de avaliação Silhouette Score. Essa métrica fornece uma indicação da qualidade do agrupamento.\n",
    "\n",
    "Os resultados, incluindo o número de clusters encontrados e a respectiva métrica de avaliação, são exibidos após a célula. Caso apenas um cluster seja identificado, uma mensagem alerta que a maioria dos pontos pode ter sido classificada como ruído. Por fim, os rótulos de clustering são exibidos para inspeção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo os parâmetros do DBSCAN\n",
    "clustering = DBSCAN(eps=7, min_samples=2)\n",
    "labels = clustering.fit_predict(df_clean)\n",
    "\n",
    "# Excluindo ruídos (label -1) para contagem de clusters\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "if n_clusters > 1:  # Calcula as métricas apenas se houver mais de 1 cluster\n",
    "    # Calcula os scores de avaliação\n",
    "    silhouette = silhouette_score(df_clean, labels)\n",
    "\n",
    "    # Formata a saída com informações claras\n",
    "    print(f\"O número de clusters encontrados foi: {n_clusters}\")\n",
    "    print(f\"Silhouette Score: {silhouette} (quanto maior, melhor [max = 1])\")\n",
    "else:\n",
    "    print(f\"Apenas {n_clusters} cluster(s) foi/foram encontrado(s). DBSCAN pode ter classificado a maioria dos pontos como ruído.\")\n",
    "\n",
    "# Exibe as labels para inspeção\n",
    "print(f\"Labels do clustering: {clustering.labels_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Finetunning de Hiperparâmetro (GridSearch)\n",
    "\n",
    "Este código define uma função para avaliar o algoritmo DBSCAN com diferentes combinações de parâmetros `eps` e `min_samples`. A função utiliza uma barra de progresso para acompanhar o andamento da avaliação em um conjunto de dados limpos. Os melhores parâmetros são atualizados com base no Davies-Bouldin Score, onde valores mais baixos indicam melhores agrupamentos.\n",
    "\n",
    "Quando ativado, o código executa uma busca em grade (*gird search*) manualmente, testando uma faixa de valores para ambos os parâmetros e exibindo os melhores resultados no final. O melhor resultado obtido foi um valor de 7 para `eps` e 2 para `min_samples`.\n",
    "\n",
    "### **ATENÇÃO!!!**\n",
    "**O código está comentado devido ao tempo de execução prolongado (pode chegar a horas dependendo do computador).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_dbscan(df_clean, eps_values, min_samples_values):\n",
    "#     best_eps = None\n",
    "#     best_min_samples = None\n",
    "#     best_score = np.inf\n",
    "#     best_n_clusters = 0\n",
    "    \n",
    "#     total_iterations = len(eps_values) * len(min_samples_values)\n",
    "#     progress_bar = tqdm(total=total_iterations, desc=\"Overall Progress\")\n",
    "    \n",
    "#     for eps in eps_values:\n",
    "#         for min_samples in min_samples_values:\n",
    "#             # Executa o DBSCAN com a combinação atual de parâmetros\n",
    "#             dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "#             labels = dbscan.fit_predict(df_clean)\n",
    "            \n",
    "#             # Exclui os pontos rotulados como ruído (-1) para contar o número de clusters\n",
    "#             n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            \n",
    "#             # Apenas considera o resultado se o número de clusters for menor que 7\n",
    "#             if 2 <= n_clusters < 10000:  # Ajuste para desconsiderar soluções triviais com 1 cluster\n",
    "#                 # Calcular o Davies-Bouldin Score (menor é melhor)\n",
    "#                 score = davies_bouldin_score(df_clean, labels)\n",
    "                \n",
    "#                 # Se o novo score for melhor, atualizamos os melhores parâmetros\n",
    "#                 if score < best_score:\n",
    "#                     best_eps = eps\n",
    "#                     best_min_samples = min_samples\n",
    "#                     best_score = score\n",
    "#                     best_n_clusters = n_clusters\n",
    "                \n",
    "#                 # Print current results\n",
    "#                 print(f\"eps: {eps}, min_samples: {min_samples}, score: {score:}, clusters: {n_clusters}\")\n",
    "            \n",
    "#             progress_bar.update(1)\n",
    "    \n",
    "#     progress_bar.close()\n",
    "#     return best_eps, best_min_samples, best_score, best_n_clusters\n",
    "\n",
    "# eps_values = np.arange(1, 80, 1)\n",
    "# min_samples_values = np.arange(2, 50, 1)\n",
    "\n",
    "\n",
    "# # Executar o grid search manual\n",
    "# print(\"Starting DBSCAN evaluation...\")\n",
    "# best_eps, best_min_samples, best_score, best_n_clusters = evaluate_dbscan(df, eps_values, min_samples_values)\n",
    "\n",
    "# # Exibir os melhores parâmetros e resultados\n",
    "# print(\"\\nFinal Results:\")\n",
    "# print(f\"Melhor eps: {best_eps}\")\n",
    "# print(f\"Melhor min_samples: {best_min_samples}\")\n",
    "# print(f\"Melhor Davies-Bouldin Score: {best_score/10:}\")\n",
    "# print(f\"Número de clusters: {best_n_clusters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização 3D do Clustering com DBSCAN\n",
    "\n",
    "Este trecho de código realiza a visualização dos resultados do clustering DBSCAN em um gráfico interativo 3D. A redução da dimensionalidade é feita utilizando a técnica de Análise de Componentes Principais (PCA). O PCA é um método que transforma dados de alta dimensão em uma forma mais simples, preservando a maior quantidade possível de informação. Em outras palavras, ele ajuda a resumir dados complexos em menos variáveis (componentes), facilitando a visualização e a interpretação.\n",
    "\n",
    "Os dados transformados em 3D são então plotados utilizando a biblioteca Plotly, onde cada ponto representa um dado do conjunto original. As cores dos marcadores são determinadas pelos rótulos gerados pelo algoritmo DBSCAN, utilizando uma escala de cores 'Plasma' para uma melhor visualização dos clusters.\n",
    "\n",
    "O layout do gráfico é configurado com títulos para os eixos e para o gráfico em si, permitindo uma apresentação clara dos resultados. Por fim, o gráfico **interativo** é exibido, permitindo ao usuário **interagir** com a visualização, **girando e ampliando** a visualização conforme necessário.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redução para 3D com PCA\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X)\n",
    "\n",
    "# Cria o gráfico interativo 3D\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adiciona os pontos dos clusters\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=X_pca_3d[:, 0], \n",
    "    y=X_pca_3d[:, 1], \n",
    "    z=X_pca_3d[:, 2], \n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=clustering.labels_, # Cores baseadas nos clusters\n",
    "        colorscale='Plasma',      # Escolhe o esquema de cores\n",
    "        opacity=0.8\n",
    "    )\n",
    "))\n",
    "\n",
    "# Configura o layout\n",
    "fig.update_layout(\n",
    "    title=\"DBSCAN Clustering in 3D with PCA\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"First PC\",\n",
    "        yaxis_title=\"Second PC\",\n",
    "        zaxis_title=\"Third PC\"\n",
    "    ),\n",
    "    width=800,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "# Mostra o gráfico interativo\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuição do Tamanho dos Clusters\n",
    "\n",
    "Este código conta o número de pontos em cada cluster gerado pelo algoritmo DBSCAN. A função `np.unique` é utilizada para identificar os rótulos únicos dos clusters e suas respectivas contagens. Em seguida, um gráfico de barras é criado para mostrar a distribuição do tamanho de cada cluster. Cada barra representa um cluster, e a altura da barra corresponde ao número de pontos que pertencem a esse cluster.\n",
    "\n",
    "O gráfico é configurado com títulos e rótulos apropriados para os eixos, permitindo uma interpretação clara da distribuição dos pontos em cada cluster. Por fim, o gráfico é exibido, facilitando a visualização do desempenho do algoritmo de clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar o número de pontos em cada cluster\n",
    "unique, counts = np.unique(clustering.labels_, return_counts=True)\n",
    "\n",
    "# Ordena os clusters e suas contagens\n",
    "sorted_indices = np.argsort(unique)\n",
    "sorted_unique = unique[sorted_indices]\n",
    "sorted_counts = counts[sorted_indices]\n",
    "\n",
    "# Plot do tamanho de cada cluster\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Define a largura das barras\n",
    "bar_width = 0.9  # Ajuste este valor conforme necessário\n",
    "plt.bar(sorted_unique, sorted_counts, width=bar_width, color='dodgerblue')\n",
    "plt.title(\"Distribuição de Sinistros por Cluster\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Número de Sinistros\")\n",
    "plt.xticks(sorted_unique)  # Garante que todos os rótulos de cluster sejam mostrados\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adição de Rótulos de Cluster ao DataFrame\n",
    "\n",
    "Este trecho de código adiciona os rótulos de cluster obtidos do algoritmo DBSCAN ao DataFrame `df_clean`, criando uma nova coluna chamada 'Cluster'. Em seguida, uma coluna 'Cluster' também é criada no DataFrame original `df`, inicializada com valores NaN.\n",
    "\n",
    "Os rótulos de cluster são então preenchidos no DataFrame original apenas para as linhas que não foram removidas como outliers. Isso é feito utilizando o índice do DataFrame limpo `df_clean`. Por fim, as linhas do DataFrame original que ainda têm valores NaN na coluna 'Cluster' são removidas, resultando em um DataFrame que agora inclui a coluna 'Cluster', mas sem as linhas que eram outliers.\n",
    "\n",
    "O resultado final é exibido, permitindo verificar as primeiras linhas do DataFrame atualizado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supondo que df seja o DataFrame original e labels seja uma lista ou array com os rótulos de cluster\n",
    "df['Cluster'] = float('nan')  # Cria a coluna 'Cluster' com NaN\n",
    "\n",
    "# Adiciona os rótulos de cluster apenas para as linhas que não foram removidas\n",
    "df.loc[df_clean.index, 'Cluster'] = labels  # Usando .loc para evitar o SettingWithCopyWarning\n",
    "\n",
    "# Remove as linhas onde 'Cluster' é NaN (ou seja, linhas que eram outliers)\n",
    "df = df.dropna(subset=['Cluster'])\n",
    "\n",
    "# Exibe as primeiras linhas do DataFrame atualizado\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de Clusters\n",
    "\n",
    "Este código define uma função chamada `analyze_cluster` que realiza uma análise das estatísticas de interesse para cada cluster presente no DataFrame. As features analisadas incluem 'Codigo Empresa Sinistro', 'Sexo Sinistro', 'Faixa-Etária Nova Sinistro', 'Descricao Plano Sinistro' e 'Valor Pago Sinistro'.\n",
    "\n",
    "A função começa identificando todos os clusters únicos e calculando a soma total do \"Valor Pago Sinistro\". Para cada cluster, são exibidas informações como o número de entradas no cluster e, para cada feature:\n",
    "\n",
    "- **Valor Pago Sinistro**: apresenta a soma absoluta e a soma relativa em relação ao valor total.\n",
    "- **Demais Features**: calcula a frequência absoluta e percentual dos valores.\n",
    "\n",
    "O resultado da análise é impresso, com uma separação clara entre os clusters, facilitando a visualização e compreensão dos dados por cluster. A função é então executada com o DataFrame `df`, permitindo uma avaliação detalhada de cada cluster.\n",
    "\n",
    "#### **ATENÇÃO!!!**\n",
    "**O código gera um output muito grande, por isso é necessário abri-lo em um formato scrolável ou em um editor de texto, que estão demarcados nas seguintes opções após a exibição parcial do output:**\n",
    "\n",
    "Output is truncated. View as a ***scrollable*** element or open in a ***text editor***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos considerar as features de interesse\n",
    "features = [\n",
    "    'Codigo Empresa Sinistro',\n",
    "    'Sexo Sinistro',\n",
    "    'Faixa-Etária Nova Sinistro',\n",
    "    'Descricao Plano Sinistro',\n",
    "    'Valor Pago Sinistro'\n",
    "]\n",
    "\n",
    "# Função para calcular estatísticas por cluster\n",
    "def analyze_cluster(df, cluster_col='Cluster'):\n",
    "    clusters = df[cluster_col].unique()\n",
    "    \n",
    "    # Calcula a soma total do \"Valor Pago Sinistro\" em todos os clusters\n",
    "    total_valor_pago = df['Valor Pago Sinistro'].sum()\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        print(f\"\\n\\n{'='*30}\")  # Linha de separação\n",
    "        print(f\"**Cluster {cluster} Analysis:**\")  # Título em negrito\n",
    "        print(f\"  ClusterID: {cluster}\")  # Indentação\n",
    "        cluster_data = df[df[cluster_col] == cluster]\n",
    "        cluster_count = len(cluster_data)\n",
    "        \n",
    "        print(f\"  Cluster_count: {cluster_count}\")\n",
    "        \n",
    "        for feature in features:\n",
    "            if feature == 'Valor Pago Sinistro':\n",
    "                # Soma absoluta e soma relativa para o Valor Pago Sinistro\n",
    "                soma_absoluta = cluster_data[feature].sum()\n",
    "                soma_relativa = (soma_absoluta / total_valor_pago) * 100\n",
    "                print(f\"\\n{feature}:\")\n",
    "                print(f\"  Soma Absoluta: {soma_absoluta:.2f}\")\n",
    "                print(f\"  Soma Relativa ao valor total: {soma_relativa:.2f}%\")\n",
    "            else:\n",
    "                # Frequência absoluta e percentual para as outras features\n",
    "                freq_abs = cluster_data[feature].value_counts()\n",
    "                freq_pct = (freq_abs / cluster_count) * 100\n",
    "                \n",
    "                print(f\"\\n{feature}:\")\n",
    "                for value, count in freq_abs.items():\n",
    "                    pct = freq_pct[value]\n",
    "                    print(f\"  {value} = {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Executar a análise\n",
    "analyze_cluster(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráfico da Soma do Valor Pago por Cluster\n",
    "\n",
    "Este código define a função `plot_sum_valor_pago`, que cria um gráfico de barras para visualizar a soma do \"Valor Pago Sinistro\" por cluster. A função utiliza a biblioteca `matplotlib` para gerar o gráfico.\n",
    "\n",
    "Primeiro, a função calcula a soma do valor pago por cluster usando o método `groupby` do `pandas`, seguido de um `reset_index` para formatar os dados corretamente. Em seguida, os clusters são ordenados com base no valor da soma em ordem decrescente.\n",
    "\n",
    "Um gráfico de barras é então gerado, aplicando o colormap `viridis` para colorir as barras. O eixo Y é rotulado com \"Soma do Valor Pago Sinistro (R$)\", e o título do gráfico é \"Soma do Valor Pago Sinistro por Cluster em Reais\".\n",
    "\n",
    "A função adiciona rótulos apenas para os três maiores valores de soma no gráfico, facilitando a identificação dos clusters mais relevantes. Por fim, os ticks do eixo X são configurados para exibir como inteiros e sem rotação, garantindo uma apresentação clara.\n",
    "\n",
    "A função é chamada com o DataFrame `df` para gerar e exibir o gráfico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar gráfico da soma do valor pago por cluster\n",
    "def plot_sum_valor_pago(df, feature='Valor Pago Sinistro', cluster_col='Cluster'):\n",
    "    # Calcula a soma do valor pago sinistro por cluster\n",
    "    sum_values = df.groupby(cluster_col)[feature].sum().reset_index()\n",
    "\n",
    "    # Ordena os clusters pelo valor da soma\n",
    "    sum_values = sum_values.sort_values(by=feature, ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Aplicar o colormap viridis para as barras\n",
    "    colors = cm.viridis(np.linspace(0, 1, len(sum_values)))\n",
    "    bars = plt.bar(sum_values[cluster_col], sum_values[feature], color=colors)\n",
    "    \n",
    "    plt.ylabel('Soma do Valor Pago Sinistro (R$)')\n",
    "    plt.xlabel(\"Cluster\")\n",
    "    plt.title('Soma do Valor Pago Sinistro por Cluster em Reais')\n",
    "\n",
    "    # Adicionar rótulos apenas para os três maiores valores\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        if bar.get_x() + bar.get_width()/2 in sum_values[cluster_col][:3].values:\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, yval, f\"{yval:,.0f}\", va='bottom', ha='center', fontsize=10)\n",
    "\n",
    "    # Definir ticks no eixo X para exibir como inteiros e sem rotação\n",
    "    plt.xticks(ticks=sum_values[cluster_col], labels=sum_values[cluster_col].astype(int), rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Chamar a função para gerar o gráfico\n",
    "plot_sum_valor_pago(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuição de Sexo por Cluster\n",
    "\n",
    "Este código calcula e plota a frequência percentual da variável \"Sexo Sinistro\" por cluster, utilizando um gráfico de barras empilhadas para facilitar a visualização das proporções.\n",
    "\n",
    "Primeiro, a distribuição de frequência é obtida agrupando os dados por \"Cluster\" e \"Sexo Sinistro\". O método `size()` conta o número de ocorrências em cada grupo, e `unstack()` transforma a série resultante em um DataFrame, onde as linhas correspondem aos clusters e as colunas representam os sexos. Valores ausentes são preenchidos com zero usando `fillna(0)`.\n",
    "\n",
    "Em seguida, a porcentagem de cada sexo dentro de cada cluster é calculada dividindo-se o número de ocorrências pelo total de ocorrências em cada cluster, multiplicando por 100 para obter a porcentagem.\n",
    "\n",
    "Um gráfico de barras empilhadas é então gerado para visualizar a distribuição percentual, utilizando as cores `lightcoral` para feminino e `lightblue` para masculino. O gráfico é rotulado com um título, e os eixos X e Y são configurados para exibir as informações de forma clara.\n",
    "\n",
    "A legenda é ajustada para identificar claramente os sexos, e linhas de grade horizontais são adicionadas para facilitar a leitura dos dados. Os rótulos do eixo X são formatados para serem inteiros e exibidos em pé.\n",
    "\n",
    "Por fim, o eixo Y é formatado para exibir os valores como porcentagens, e o gráfico é exibido com o layout ajustado para uma melhor apresentação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequência percentual de Sexo Sinistro por Cluster\n",
    "sexo_dist = df.groupby(['Cluster', 'Sexo Sinistro']).size().unstack().fillna(0)\n",
    "\n",
    "# Calcular a porcentagem dentro de cada cluster\n",
    "sexo_dist_percentage = sexo_dist.div(sexo_dist.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Plotar o gráfico com as barras empilhadas\n",
    "sexo_dist_percentage.plot(kind='bar', stacked=True, figsize=(12, 7), color=['lightcoral', 'lightblue'])\n",
    "\n",
    "# Ajustar o título e rótulos\n",
    "plt.title('Distribuição de Sexo por Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Frequência Percentual')\n",
    "\n",
    "# Ajustar a legenda para \"Masculino\" e \"Feminino\"\n",
    "plt.legend(['Feminino', 'Masculino'], title='Sexo')\n",
    "\n",
    "# Adicionar linhas de grade horizontais\n",
    "plt.grid(axis='y', color=\"lightgray\")\n",
    "\n",
    "# Ajustar os rótulos do eixo x para ficarem de pé e serem inteiros\n",
    "plt.xticks(ticks=np.arange(len(sexo_dist_percentage.index)), \n",
    "           labels=sexo_dist_percentage.index.astype(int), rotation=0)\n",
    "\n",
    "# Formatar o eixo y para exibir como porcentagem\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter())\n",
    "\n",
    "# Exibir o gráfico\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuição de Faixa Etária por Cluster\n",
    "\n",
    "Este código calcula e plota a frequência percentual da variável \"Faixa-Etária Nova Sinistro\" por cluster, utilizando um gráfico de barras empilhadas para ilustrar a proporção de cada faixa etária dentro dos diferentes clusters.\n",
    "\n",
    "Primeiro, é feita uma cópia do DataFrame `df_clean` e uma nova coluna chamada \"Cluster\" é adicionada a ele, que contém os rótulos obtidos do modelo de clustering.\n",
    "\n",
    "Em seguida, a distribuição de frequência é calculada agrupando os dados por \"Cluster\" e \"Faixa-Etária Nova Sinistro\". O método `size()` conta o número de ocorrências em cada combinação de cluster e faixa etária, enquanto `unstack(fill_value=0)` transforma a série resultante em um DataFrame onde as linhas correspondem aos clusters e as colunas representam as faixas etárias. O parâmetro `fill_value=0` garante que clusters sem nenhuma ocorrência em uma faixa etária sejam representados com zero.\n",
    "\n",
    "Para calcular a porcentagem de cada faixa etária dentro de cada cluster, os valores são normalizados dividindo-se o número de ocorrências de cada faixa etária pelo total de ocorrências no respectivo cluster, multiplicando por 100 para obter a porcentagem.\n",
    "\n",
    "Um gráfico de barras empilhadas é gerado para visualizar a distribuição percentual, utilizando uma paleta de cores do `tab10` do Matplotlib. O gráfico é rotulado com um título, e os eixos X e Y são configurados para apresentar as informações de forma clara.\n",
    "\n",
    "A legenda é ajustada para exibir as faixas etárias de 0 a 9 e posicionada fora do gráfico para melhor visualização. Linhas de grade horizontais são adicionadas para facilitar a leitura dos dados, e os rótulos do eixo X são formatados para serem inteiros e exibidos na horizontal.\n",
    "\n",
    "Por fim, o eixo Y é formatado para mostrar os valores como porcentagens, e o gráfico é exibido com o layout ajustado para uma melhor apresentação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supondo que df_with_labels e labels já estejam definidos como no seu código anterior.\n",
    "\n",
    "df_with_labels = df_clean.copy()\n",
    "df_with_labels['Cluster'] = labels  # Adiciona a coluna com os labels  \n",
    "\n",
    "# Frequência percentual de Faixa Etária por Cluster\n",
    "faixa_etaria_dist = df_with_labels.groupby(['Cluster', 'Faixa-Etária Nova Sinistro']).size().unstack(fill_value=0)\n",
    "\n",
    "# Calcular a porcentagem dentro de cada cluster\n",
    "faixa_etaria_dist_percentage = faixa_etaria_dist.div(faixa_etaria_dist.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Garantir que as faixas etárias de 0 a 9 sejam incluídas na legenda\n",
    "faixa_etaria_labels = np.arange(10)  # Criar uma lista de 0 a 9 para a legenda\n",
    "\n",
    "# Plotar o gráfico com as barras empilhadas\n",
    "faixa_etaria_dist_percentage.plot(kind='bar', stacked=True, figsize=(12, 7), color=plt.cm.tab10(np.arange(len(faixa_etaria_dist_percentage.columns))))\n",
    "\n",
    "# Ajustar o título e rótulos\n",
    "plt.title('Distribuição de Faixa Etária por Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Frequência Percentual')\n",
    "\n",
    "# Ajustar a legenda para exibir de 0 a 9 e posicioná-la fora do gráfico\n",
    "plt.legend(faixa_etaria_labels, title='Faixa Etária', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Adicionar linhas de grade horizontais\n",
    "plt.grid(axis='y', color=\"lightgray\")\n",
    "\n",
    "# Ajustar os rótulos do eixo x para ficarem de pé e serem inteiros\n",
    "plt.xticks(ticks=np.arange(len(faixa_etaria_dist_percentage.index)),\n",
    "           labels=faixa_etaria_dist_percentage.index.astype(int), rotation=0)\n",
    "\n",
    "# Formatar o eixo y para exibir como porcentagem\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter())\n",
    "\n",
    "# Exibir o gráfico\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook implementa o algoritmo de clustering DBSCAN (Density-Based Spatial Clustering of Applications with Noise) para analisar um conjunto de dados sobre sinistros. O processo começa com a definição dos parâmetros do DBSCAN, como `eps` e `min_samples`, que são ajustados para otimizar a formação de clusters. Após a execução do algoritmo, são realizadas análises estatísticas e visuais dos resultados, incluindo a contagem de pontos em cada cluster, a soma do valor pago por sinistros e a distribuição de variáveis categóricas, como sexo e faixa etária, entre os clusters formados.\n",
    "\n",
    "A análise detalhada dos resultados permite a identificação de padrões e comportamentos dentro dos grupos, contribuindo para uma melhor compreensão dos dados. Essa abordagem valida a eficácia do DBSCAN na identificação de estruturas densas nos dados e fornece insights práticos que podem ser utilizados em estratégias de gerenciamento de sinistros, desenvolvimento de políticas e aprimoramento dos serviços oferecidos pela Unipar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Cálculo de métricas\n",
    "\n",
    "Aqui é realizado o cálculo das métricas escolhidas: Silhouette Score, Davis-Bouldin Index e Calinski-Harabasz Index. Elas ajudam a verificar se a formação dos *clusters* foi bem-sucedida, com base na qualidade da segmentação obtida. O resultado dessas métricas sozinho não deve ser a única maneira de avaliar se o modelo de fato é o melhor, estamos levando em consideração como os *clusters* foram segmentados pelo algoritmo para ter certeza de que as divisões feitas realmente fazem sentido para o projeto.\n",
    "\n",
    "- O Davies-Bouldin Score deve ser minimizado (mínimo possível é 0).\n",
    "- O Silhouette Score deve ser maximizado (máximo possível é 1).\n",
    "- O Calinski-Harabasz Score também deve ser maximizado (não há um limite superior).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo os parâmetros do DBSCAN\n",
    "clustering = DBSCAN(eps=7, min_samples=2)\n",
    "labels = clustering.fit_predict(df_clean)\n",
    "\n",
    "# Excluindo ruídos (label -1) para contagem de clusters\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "if n_clusters > 1:  # Calcula as métricas apenas se houver mais de 1 cluster\n",
    "    # Calcula os scores de avaliação\n",
    "    db_score = davies_bouldin_score(df_clean, labels)\n",
    "    silhouette = silhouette_score(df_clean, labels)\n",
    "    calinski = calinski_harabasz_score(df_clean, labels)\n",
    "\n",
    "    # Formata a saída com informações claras\n",
    "    print(f\"Davies-Bouldin Score: {db_score} (quanto menor, melhor [min = 0])\")\n",
    "    print(f\"Silhouette Score: {silhouette} (quanto maior, melhor [max = 1])\")\n",
    "    print(f\"Calinski-Harabasz Score: {calinski} (quanto maior, melhor [max = inf])\")\n",
    "else:\n",
    "    print(f\"Apenas {n_clusters} cluster(s) foi/foram encontrado(s). DBSCAN pode ter classificado a maioria dos pontos como ruído.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
